{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DVL7_bgmIAPR"
      },
      "source": [
        "# Backpropagation Lab\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_a2KSZ_7AN0G"
      },
      "outputs": [],
      "source": [
        "# from typing_extensions import ParamSpecKwargs\n",
        "# class MLP(BaseEstimator,ClassifierMixin):\n",
        "\n",
        "#     def __init__(self, epochs, lr=.1, momentum=0, shuffle=True,hidden_layer_widths=None, num_output_nodes=1):\n",
        "#         \"\"\" Initialize class with chosen hyperparameters.\n",
        "\n",
        "#         Args:\n",
        "#             lr (float): A learning rate / step size.\n",
        "#             shuffle(boolean): Whether to shuffle the training data each epoch. DO NOT SHUFFLE for evaluation / debug datasets.\n",
        "#             momentum(float): The momentum coefficent \n",
        "#         Optional Args (Args we think will make your life easier):\n",
        "#             hidden_layer_widths (list(int)): A list of integers which defines the width of each hidden layer if hidden layer is none do twice as many hidden nodes as input nodes. (and then one more for the bias node)\n",
        "#             For example: input width 1, then hidden layer will be 3 nodes\n",
        "#         Example:\n",
        "#             mlp = MLP(lr=.2,momentum=.5,shuffle=False,hidden_layer_widths = [3,3]),  <--- this will create a model with two hidden layers, both 3 nodes wide\n",
        "#         \"\"\"\n",
        "#         self.epochs = epochs\n",
        "#         self.num_ouput_nodes = num_output_nodes\n",
        "#         self.hidden_layer_widths = hidden_layer_widths\n",
        "#         self.lr = lr\n",
        "#         self.momentum = momentum\n",
        "#         self.shuffle = shuffle\n",
        "#         self.layer_wts =[] # list of numpy matrices or vectors with the first element being the weights between the inputs and the first hidden layer, the second element being the weights between the first and second hidden layers, etc.\n",
        "\n",
        "\n",
        "#     def err_at_node(self, output, E_q, train_data_row, o_q):\n",
        "#       return E_q*o_q*(1-o_q)\n",
        "\n",
        "#     def fit(self, X, y, epochs_no_change=11, tol=0.01, initial_weights_zero=False):\n",
        "#         \"\"\" Fit the data; run the algorithm and adjust the weights to find a good solution\n",
        "\n",
        "#         Args:\n",
        "#             X (array-like): A 2D numpy array with the training data, excluding targets\n",
        "#             y (array-like): A 2D numpy array with the training targets\n",
        "#         Optional Args (Args we think will make your life easier):\n",
        "#             initial_weights (array-like): allows the user to provide initial weights\n",
        "#         Returns:\n",
        "#             self: this allows this to be chained, e.g. model.fit(X,y).predict(X_test)\n",
        "\n",
        "#         \"\"\"\n",
        "#         self.X = X\n",
        "#         self.y = y\n",
        "#         # if type(initial_weights_zero) != np.ndarray:\n",
        "#         #   self.layer_wts = self.initialize_weights()\n",
        "#         # else:\n",
        "#         #   self.layer_wts = initial_weights\n",
        "#         if initial_weights_zero:\n",
        "#           self.layer_wts = self.initialize_zero_weights()\n",
        "#         else:\n",
        "#           self.layer_wts = self.initialize_weights()\n",
        "#         print(\"initial weights:\\n\", self.layer_wts)\n",
        "\n",
        "#         # for each instance of training data:\n",
        "#         #   feedForward\n",
        "#         #   backPropagate\n",
        "\n",
        "#         self.cur_epoch = 0\n",
        "#         num_epoch_no_change = 0\n",
        "#         last_score = None\n",
        "#         while self.cur_epoch < self.epochs and num_epoch_no_change < epochs_no_change:\n",
        "#           X_bias = None\n",
        "#           if self.shuffle:\n",
        "#             self._shuffle_data(self.X, self.y)\n",
        "#           bias = np.ones((self.X.shape[0],1)) # bias vector of ones\n",
        "#           X_bias = np.concatenate((self.X,bias), axis=1) # set X = X_shuffled with bias concatenated\n",
        "#           #---------------\n",
        "#           v_sigmoid = np.vectorize(sigmoid)\n",
        "#           self.Zs_by_layer = []\n",
        "#           self.nets_by_layer = []\n",
        "#           for row in range(0, X_bias.shape[0]):\n",
        "#             # FORWARD-FEED\n",
        "#             for lay in range(0, len(self.layer_wts)):\n",
        "#               if lay == 0:\n",
        "#                 print('X_bias[row] = \\n', X_bias[row])\n",
        "#                 print(f'layer_wts at layer {lay} = \\n', self.layer_wts[lay])\n",
        "#                 net = np.matmul(X_bias[row], self.layer_wts[lay])\n",
        "#                 print(f'net at layer{lay}:\\n', net)\n",
        "#                 self.nets_by_layer.append(net)\n",
        "#                 Zs_lay = v_sigmoid(net)\n",
        "#                 print(f'Zs at layer {lay}', Zs_lay)\n",
        "#                 self.Zs_by_layer.append(Zs_lay)\n",
        "#               else:\n",
        "#                 print('\\nX_bias[row] = \\n', X_bias[row])\n",
        "#                 self.Zs_by_layer[lay-1] = np.insert(self.Zs_by_layer[lay-1],self.Zs_by_layer[lay-1].shape[0],[1]) # add a one to the Zs_by_layer for the bias\n",
        "#                 print(f'Zs at layer{lay}\\n', self.Zs_by_layer[lay-1]) # this has to be lay - 1 because each element is added per iteration\n",
        "#                 print(f'layer_wts at layer{lay}\\n', self.layer_wts[lay]) # self.layer_wts[lay][:-1,:] this would get all els except the last one\n",
        "#                 net = np.matmul(self.Zs_by_layer[lay-1], self.layer_wts[lay])\n",
        "#                 print(f'net at layer{lay}:\\n', net)\n",
        "#                 self.nets_by_layer.append(net)\n",
        "#                 Zs_lay = v_sigmoid(net)\n",
        "#                 print(f'Zs at layer {lay}\\n', Zs_lay)\n",
        "#                 self.Zs_by_layer.append(Zs_lay)   \n",
        "#             # BACK PROPAGATE\n",
        "#             for lay_indx in reversed(range(0, len(self.layer_wts))):\n",
        "#               for node_indx in range(0,len(self.hidden_layer_widths)):\n",
        "#                 pass\n",
        "              \n",
        "#           #---------------\n",
        "            \n",
        "#             net = self.calcNet(X_bias, row)\n",
        "#             output = self.output(net)\n",
        "#             delta_wts = self.deltaWts(X_bias, row, output)\n",
        "#             self.weights = np.add(self.weights, delta_wts)\n",
        "\n",
        "#           if self.cur_epoch == 0:\n",
        "#             last_score = self.score(self.X,self.y)\n",
        "#             self.accuracies.append(last_score)\n",
        "#           elif self.cur_epoch > 0:\n",
        "#             cur_score = self.score(self.X,self.y)\n",
        "#             if ((cur_score - last_score)**2)**(1/2) < tol:\n",
        "#               num_epoch_no_change +=1\n",
        "#             else:\n",
        "#               num_epoch_no_change = 0\n",
        "#             last_score = cur_score\n",
        "#             self.accuracies.append(last_score)\n",
        "          \n",
        "#           self.cur_epoch +=1\n",
        "        \n",
        "#         return self\n",
        "\n",
        "  \n",
        "#     def calcNet(self, X, row):\n",
        "#       return np.dot(X[row], self.weights)\n",
        "\n",
        "#     def output(self, net):\n",
        "#       if net > 0:\n",
        "#         return 1\n",
        "#       else:\n",
        "#         return 0  \n",
        "\n",
        "#     def deltaWts(self, X, row, output):\n",
        "#       \"\"\"\n",
        "#       calculates the delta_wts for the given row then returns those wts\n",
        "#       \"\"\"\n",
        "#       num_cols = X.shape[1]\n",
        "#       delta_wts = np.zeros([self.weights.shape[0],1])\n",
        "#       for col in range(0, num_cols):\n",
        "#         delta_wts[col][0] = self.lr *(self.y[row][0] - output)*X[row][col]\n",
        "#       return delta_wts\n",
        "\n",
        "#     def delta_node_hidden():\n",
        "#       # for each node k that node j feeds:\n",
        "#         # (sum up the weight from j to k times the delta of node k)*z_j*(1-z_j)\n",
        "\n",
        "#         # returns the delta of a single node\n",
        "#         pass\n",
        "\n",
        "#     def delta_node_output(self,row):\n",
        "#       t = self.y[row][0]\n",
        "#       # delta_node = (Target - z_j)*z_J*(1-z_j)\n",
        "#       # return delta_node\n",
        "#       pass\n",
        "\n",
        "#     def predict(self, X):\n",
        "#         \"\"\" Predict all classes for a dataset X\n",
        "#         Args:\n",
        "#             X (array-like): A 2D numpy array with the training data, excluding targets\n",
        "#         Returns:\n",
        "#             array, shape (n_samples,)\n",
        "#                 Predicted target values per element in X.\n",
        "#         \"\"\"\n",
        "#         pass\n",
        "\n",
        "#     def initialize_weights(self):\n",
        "#         \"\"\" Initialize weights for perceptron. Don't forget the bias!\n",
        "\n",
        "#         Returns:\n",
        "\n",
        "#         \"\"\"\n",
        "#         #---FIRST SET OF WEIGHTS---#\n",
        "#         # the first set of weights (between the input nodes and the first hidden layer) have a dimension of nxm,\n",
        "#           # where n = the number of inputs + 1 (+1 for bias), and m = the number of nodes in the first hidden layer (width of the layer) \n",
        "#         self.layer_wts.append(np.random.randn(self.X.shape[1] + 1, self.hidden_layer_widths[0]))\n",
        "#         # print(np.random.randn(self.X.shape[1] + 1, self.hidden_layer_widths[0]))\n",
        "        \n",
        "#         num_wt_vectors = len(self.hidden_layer_widths)\n",
        "#         for i in range(0, num_wt_vectors):\n",
        "#           #--MIDDLE SET OF WEIGHTS--#\n",
        "#           # the middle set of weights (between the hidden layers) have a dimension of nxm,\n",
        "#             # where n = the number of nodes at the 'from' layer + 1 (+1 for bias), and m = the number of nodes in the 'to' hidden layer (width of the layer)\n",
        "#           if i < num_wt_vectors - 1:\n",
        "#             self.layer_wts.append(np.random.randn(self.hidden_layer_widths[i]+1, self.hidden_layer_widths[i]))\n",
        "#             # print(np.random.randn(self.hidden_layer_widths[i]+1, self.hidden_layer_widths[i]))\n",
        "#           else:\n",
        "#           #---LAST SET OF WEIGHTS---#\n",
        "#           # the last set of weights (between the last hidden layer and the output node) have a dimension of nxm,\n",
        "#             # where n = the number of nodes at the 'from' layer + 1 (+1 for bias) and m is the number of output nodes\n",
        "#             self.layer_wts.append(np.random.randn(self.hidden_layer_widths[i]+1, self.num_ouput_nodes))\n",
        "#             # print(np.random.randn(self.hidden_layer_widths[i]+1,self.num_ouput_nodes))\n",
        "#         return self.layer_wts\n",
        "\n",
        "#     def initialize_zero_weights(self):\n",
        "#         \"\"\" Initialize weights for perceptron. Don't forget the bias!\n",
        "\n",
        "#         Returns:\n",
        "\n",
        "#         \"\"\"\n",
        "#         #---FIRST SET OF WEIGHTS---#\n",
        "#         # the first set of weights (between the input nodes and the first hidden layer) have a dimension of nxm,\n",
        "#           # where n = the number of inputs + 1 (+1 for bias), and m = the number of nodes in the first hidden layer (width of the layer) \n",
        "#         self.layer_wts.append(np.zeros((self.X.shape[1] + 1, self.hidden_layer_widths[0])))\n",
        "#         # print(np.zeros((self.X.shape[1] + 1, self.hidden_layer_widths[0])))\n",
        "        \n",
        "#         num_wt_vectors = len(self.hidden_layer_widths)\n",
        "#         for i in range(0, num_wt_vectors):\n",
        "#           #--MIDDLE SET OF WEIGHTS--#\n",
        "#           # the middle set of weights (between the hidden layers) have a dimension of nxm,\n",
        "#             # where n = the number of nodes at the 'from' layer + 1 (+1 for bias), and m = the number of nodes in the 'to' hidden layer (width of the layer)\n",
        "#           if i < num_wt_vectors - 1:\n",
        "#             self.layer_wts.append(np.zeros((self.hidden_layer_widths[i]+1, self.hidden_layer_widths[i])))\n",
        "#             # print(np.zeros((self.hidden_layer_widths[i]+1, self.hidden_layer_widths[i])))\n",
        "#           else:\n",
        "#           #---LAST SET OF WEIGHTS---#\n",
        "#           # the last set of weights (between the last hidden layer and the output node) have a dimension of nxm,\n",
        "#             # where n = the number of nodes at the 'from' layer + 1 (+1 for bias) and m is the number of output nodes\n",
        "#             self.layer_wts.append(np.zeros((self.hidden_layer_widths[i]+1, self.num_ouput_nodes)))\n",
        "#             # print(np.zeros((self.hidden_layer_widths[i]+1,self.num_ouput_nodes)))\n",
        "#         return self.layer_wts\n",
        "\n",
        "\n",
        "#     def score(self, X, y):\n",
        "#         \"\"\" Return accuracy of model on a given dataset. Must implement own score function.\n",
        "\n",
        "#         Args:\n",
        "#             X (array-like): A 2D numpy array with data, excluding targets\n",
        "#             y (array-like): A 2D numpy array with targets\n",
        "\n",
        "#         Returns:\n",
        "#             score : float\n",
        "#                 Mean accuracy of self.predict(X) wrt. y.\n",
        "#         \"\"\"\n",
        "\n",
        "#         return 0\n",
        "\n",
        "#     def _shuffle_data(self, X, y):\n",
        "#         \"\"\" Shuffle the data! This _ prefix suggests that this method should only be called internally.\n",
        "#             It might be easier to concatenate X & y and shuffle a single 2D array, rather than\n",
        "#              shuffling X and y exactly the same way, independently.\n",
        "#         \"\"\"\n",
        "#         pass\n",
        "\n",
        "#     ### Not required by sk-learn but required by us for grading. Returns the weights.\n",
        "#     def get_weights(self):\n",
        "#         return self.init_layer_wts\n",
        "\n",
        "\n",
        "#     # def init_layer_wts(self, X):\n",
        "#     #   num_wt_vectors = len(self.hidden_layer_widths)\n",
        "#     #   self.layer_wts.append(np.zeros((X.shape[1] + 1, self.hidden_layer_widths[0])))\n",
        "#     #   print(np.zeros((X.shape[1] + 1, self.hidden_layer_widths[0])))\n",
        "#     #   for i in range(0, num_wt_vectors):\n",
        "#     #     if i < num_wt_vectors - 1:\n",
        "#     #       self.layer_wts.append(np.zeros((self.hidden_layer_widths[i]+1, self.hidden_layer_widths[i])))\n",
        "#     #       print(np.zeros(self.hidden_layer_widths[i]+1, self.hidden_layer_widths[i+1]))\n",
        "#     #     else:\n",
        "#     #       self.layer_wts.append(np.zeros((self.hidden_layer_widths[i]+1, 1)))\n",
        "#     #       print(np.zeros(self.hidden_layer_widths[i]+1))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Debug Data = $\\begin{bmatrix} -0.4 & 0.3 & 1 \\\\ -0.3 & 0.8 & 1\\\\ \n",
        "-0.2 & 0.3 & 1\\\\ -0.1 & 0.9 & 1\\\\ -0.1 & 0.1 & 0\\\\ 0.0 & -0.2 & 0\\\\ 0.1 & 0.2 & 0\\\\ 0.2 & -0.2 & 0\\end{bmatrix}$\n",
        "\n",
        "$net_{node} = w_{layer} \\cdot x$\n",
        "\n",
        "$z_{node} = \\frac{1}{1+e^{net}}$\n",
        "\n",
        "$\\delta_{w_{pq}} = $"
      ],
      "metadata": {
        "id": "xnocHDpkTCv7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "6ZbYjZZZ_yLV"
      },
      "outputs": [],
      "source": [
        "from sklearn.base import BaseEstimator, ClassifierMixin\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EWMWTgeiJWJ4",
        "outputId": "5e5f8b5d-6c80-4856-88b0-83409fa9846d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sCcEPx5VIORj"
      },
      "source": [
        "## 1. (40%) Correctly implement and submit your own code for the backpropagation algorithm. \n",
        "\n",
        "## Code requirements \n",
        "- Ability to create a network structure with at least one hidden layer and an arbitrary number of nodes.\n",
        "- Random weight initialization with small random weights with mean of 0 and a variance of 1.\n",
        "- Use Stochastic/On-line training updates: Iterate and update weights after each training instance (i.e., do not attempt batch updates)\n",
        "- Implement a validation set based stopping criterion.\n",
        "- Shuffle training set at each epoch.\n",
        "- Option to include a momentum term\n",
        "\n",
        "Use your Backpropagation algorithm to solve the Debug data. We provide you with several parameters, and you should be able to replicate our results every time. When you are confident it is correct, run your script on the Evaluation data with the same parameters, and print your final weights."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def sigmoid(net):\n",
        "  \"\"\"The sigmoid function.\"\"\"\n",
        "  return 1.0/(1.0+np.exp(-net))"
      ],
      "metadata": {
        "id": "USpniuxk9tDx"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from os import get_terminal_size\n",
        "from typing_extensions import ParamSpecKwargs\n",
        "class MLP(BaseEstimator,ClassifierMixin):\n",
        "\n",
        "    def __init__(self, arch, epochs, lr=.1, momentum=0, shuffle=True,):\n",
        "        \"\"\" Initialize class with chosen hyperparameters.\n",
        "\n",
        "        Args:\n",
        "            arch: a list of nodes at each layer, including the inputs and outputs, but not including bias nodes\n",
        "            epochs: the number of epochs to run the fit function \n",
        "            lr (float): A learning rate / step size.\n",
        "            shuffle(boolean): Whether to shuffle the training data each epoch. DO NOT SHUFFLE for evaluation / debug datasets.\n",
        "            momentum(float): The momentum coefficent \n",
        "        Example:\n",
        "            mlp = MLP(lr=.2,momentum=.5,shuffle=False,hidden_layer_widths = [3,3]),  <--- this will create a model with two hidden layers, both 3 nodes wide\n",
        "        \"\"\"\n",
        "        self.arch = arch # [2,2,1] means 2 inputs, 1 hidden layer with 2 nodes, and 1 output\n",
        "        self.epochs = epochs\n",
        "        self.lr = lr\n",
        "        self.momentum = momentum\n",
        "        self.shuffle = shuffle\n",
        "        self.layer_wts =[] # list of numpy matrices or vectors with the first element being the weights between the inputs and the first hidden layer, the second element being the weights between the first and second hidden layers, etc.\n",
        "        self.layer_prev_wts = None\n",
        "        self.accuracies = []\n",
        "        self.node_errors_by_layer = []\n",
        "        self.delta_wts = []\n",
        "\n",
        "    def fit(self, X, y, epochs_no_change=11, tol=0.01, default_wts=False, initial_wts=0):\n",
        "        \"\"\" Fit the data; run the algorithm and adjust the weights to find a good solution\n",
        "\n",
        "        Args:\n",
        "            X (array-like): A 2D numpy array with the training data, excluding targets\n",
        "            y (array-like): A 2D numpy array with the training targets\n",
        "        Optional Args (Args we think will make your life easier):\n",
        "            initial_weights (array-like): allows the user to provide initial weights\n",
        "        Returns:\n",
        "            self: this allows this to be chained, e.g. model.fit(X,y).predict(X_test)\n",
        "\n",
        "        \"\"\"\n",
        "        self.X = X\n",
        "        self.y = y\n",
        "\n",
        "        v_sigmoid = np.vectorize(sigmoid) # this converts the sigmoid function to a function that that can take a vector and apply the function to it\n",
        "\n",
        "        if default_wts:\n",
        "          self.layer_wts = self.initialize_weights()\n",
        "        else:\n",
        "          self.layer_wts = self.initialize_weights_with_value(initial_wts)\n",
        "        self.delta_wts = self.layer_wts.copy()\n",
        "        print(\"INITIAL WEIGHTS:\\n\", self.layer_wts)\n",
        "\n",
        "        #--------------weight initialization working correctly------------------\n",
        "\n",
        "        self.cur_epoch = 0\n",
        "        num_epoch_no_change = 0\n",
        "        last_score = None\n",
        "        while self.cur_epoch < self.epochs and num_epoch_no_change < epochs_no_change:\n",
        "          X_bias = None # this will be the input matrix with a column of 1s concatenated for the bias\n",
        "          if self.shuffle:\n",
        "            self._shuffle_data(self.X, self.y)\n",
        "          bias = np.ones((self.X.shape[0],1)) # bias vector of ones\n",
        "          X_bias = np.concatenate((self.X,bias), axis=1) # set X = X_shuffled with bias concatenated\n",
        "          #---------------------------------------------------------------------\n",
        "          self.Zs_by_layer = []\n",
        "          self.nets_by_layer = []\n",
        "          for row in range(X_bias.shape[0]):\n",
        "            # FORWARD-FEED\n",
        "            print('\\n\\nFORWARD-FEED:\\n')\n",
        "            print('Training Instance data:\\n', X_bias[row])\n",
        "            for lay in range(0, len(self.layer_wts)):\n",
        "              print(f'weights at layer {lay}:\\n', self.layer_wts[lay])\n",
        "              print(f'target value for training instance #{row+1}:\\n', self.y[row][0])\n",
        "              if lay == 0:\n",
        "                net = np.matmul(X_bias[row], self.layer_wts[lay])\n",
        "                print(f'nets at hidden layer {lay+1}:\\n', net)\n",
        "                self.nets_by_layer.append(net)\n",
        "                Zs_lay = v_sigmoid(net)\n",
        "                print(f'Zs at hidden layer {lay+1}:\\n', Zs_lay)\n",
        "                self.Zs_by_layer.append(Zs_lay)\n",
        "              else:\n",
        "                self.Zs_by_layer[lay-1] = np.insert(self.Zs_by_layer[lay-1],self.Zs_by_layer[lay-1].shape[0],[1]) # add a one to the Zs_by_layer for the bias\n",
        "                print(f'Zs at hidden layer{lay+1}:\\n', self.Zs_by_layer[lay-1]) # this has to be lay - 1 because each element is added per iteration\n",
        "                # print(f'weights at layer{lay+1}:\\n', self.layer_wts[lay]) # self.layer_wts[lay][:-1,:] this would get all els except the last one\n",
        "                net = np.matmul(self.Zs_by_layer[lay-1], self.layer_wts[lay])\n",
        "                print(f'nets at hidden layer{lay+1}:\\n', net)\n",
        "                self.nets_by_layer.append(net)\n",
        "                Zs_lay = v_sigmoid(net)\n",
        "                print(f'Zs at hidden layer {lay+1}:\\n', Zs_lay)\n",
        "                self.Zs_by_layer.append(Zs_lay)   \n",
        "            # BACK PROPAGATE\n",
        "            print('\\n\\nBACK-PROPAGATE:\\n')\n",
        "            target_vec = self.get_target_vec(row)\n",
        "            target_vec = target_vec.flatten()\n",
        "            print('TARGET VECTOR:\\n', target_vec)\n",
        "            for lay_indx in reversed(range(0, len(self.layer_wts))):\n",
        "              i = 0 # need this to access the node_erros from previous layer to calculate next layer (working backwards)\n",
        "              if lay_indx == (len(self.layer_wts) - 1): \n",
        "                # COMPUTE THE ERROR AT THE OUTPUT NODES\n",
        "                  # (target_j - z_j)*z_j*(1 - z_j)\n",
        "                OutNodes_error = (target_vec - self.Zs_by_layer[lay_indx])*self.Zs_by_layer[lay_indx]*(np.ones(self.Zs_by_layer[lay_indx].shape) - self.Zs_by_layer[lay_indx])\n",
        "                # ADD THE ERROR AT OUTPUT NODES VECTOR TO node_errors_by_layer\n",
        "                self.node_errors_by_layer.append(OutNodes_error)\n",
        "                print('node erros at output layer:\\n', self.node_errors_by_layer)\n",
        "              else:\n",
        "                # CALCULATE CHANGE IN WEIGHTS\n",
        "                  # delta_wts_ij = learning_rate * error_at_node_j * z_i\n",
        "                lr_vec_shape = self.node_errors_by_layer[i-1].shape\n",
        "                lr_vec = np.full(lr_vec_shape, self.lr)\n",
        "                Zs = self.Zs_by_layer[lay_indx]\n",
        "                Zs = Zs.reshape(Zs.shape[0],1)\n",
        "                errors_at_nodes = self.node_errors_by_layer[i-1]\n",
        "                self.delta_wts[lay_indx+1] = Zs * lr_vec * errors_at_nodes \n",
        "                print('\\nself.delta_wts:\\n', self.delta_wts)\n",
        "\n",
        "                # COMPUTE THE ERROR AT HIDDEN LAYERS\n",
        "                  # error_node_j = Sum(error_node_k * wt_j->k)*f_prime_net_j\n",
        "                delta_dot_w = np.matmul(self.layer_wts[lay_indx+1],self.node_errors_by_layer[i-1])\n",
        "                delta_dot_w = delta_dot_w[:-1] # exclude the bias node\n",
        "                print('delta_dot_w = \\n', delta_dot_w)\n",
        "                  # f_prime_net = z_j*(1 - z_j)\n",
        "                f_prime_net = (self.Zs_by_layer[lay_indx]) * (np.ones( (self.Zs_by_layer[lay_indx].shape[0]) ) - self.Zs_by_layer[lay_indx]) \n",
        "                f_prime_net = f_prime_net[:-1] # exclude the bias node\n",
        "                print('f_prime_net', f_prime_net)\n",
        "                # ADD THE ERROR AT HIDDEN LAYER NODES VECTOR TO node_errors_by_layer\n",
        "                self.node_errors_by_layer.append(np.multiply(delta_dot_w ,f_prime_net))\n",
        "                print('self.node_errors_by_layer\\n', self.node_errors_by_layer)         \n",
        "              i += 1\n",
        "            \n",
        "            self.node_errors_by_layer.reverse() \n",
        "            print('SECOND\\n',self.node_errors_by_layer)\n",
        "          #---------------\n",
        "\n",
        "          # if self.cur_epoch == 0:\n",
        "          #   last_score = self.score(self.X,self.y)\n",
        "          #   self.accuracies.append(last_score)\n",
        "          # elif self.cur_epoch > 0:\n",
        "          #   cur_score = self.score(self.X,self.y)\n",
        "          #   if ((cur_score - last_score)**2)**(1/2) < tol:\n",
        "          #     num_epoch_no_change +=1\n",
        "          #   else:\n",
        "          #     num_epoch_no_change = 0\n",
        "          #   last_score = cur_score\n",
        "          #   self.accuracies.append(last_score)\n",
        "          \n",
        "          self.cur_epoch +=1\n",
        "        \n",
        "        return self\n",
        "\n",
        "    def get_target_vec(self,row):\n",
        "      # THIS RETURNS A COLUMN VECTOR\n",
        "      target_value = int(self.y[row][0])\n",
        "      target_vector = np.zeros(shape=(self.arch[-1],1))\n",
        "      target_vector[target_value][0] = 1\n",
        "      return target_vector\n",
        "      \n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\" \n",
        "            Predict all classes for a dataset X\n",
        "        Args:\n",
        "            X (array-like): A 2D numpy array with the training data, excluding \n",
        "            targets\n",
        "        Returns:\n",
        "            array, shape (n_samples,)\n",
        "                Predicted target values per element in X.\n",
        "        \"\"\"\n",
        "        # bias = np.ones((X.shape[0],1)) # bias vector of ones\n",
        "        # X_bias = np.concatenate((X,bias), axis=1) # set X = X_shuffled with bias concatenated\n",
        "        # predictions = np.zeros([X.shape[0],1])\n",
        "        # for row in range(0, X.shape[0]):\n",
        "        #   net = self.calcNet(X_bias, row)\n",
        "        #   output = self.output(net)\n",
        "        #   predictions[row][0] = output\n",
        "        # return predictions\n",
        "        return 0\n",
        "\n",
        "    def initialize_weights_with_value(self, val):\n",
        "      # if self.arch = [2,3,2] then the lay1_wts have a shape of (3,3) and lay2_wts a shape of (4,2)\n",
        "        for i in range(0, len(self.arch) - 1):\n",
        "          self.layer_wts.append(np.full((self.arch[i] + 1, self.arch[i+1]),val))\n",
        "        return self.layer_wts\n",
        "\n",
        "    def initialize_weights(self):\n",
        "        \"\"\" Initialize weights for perceptron. Don't forget the bias!\n",
        "        Returns:\n",
        "        \"\"\"\n",
        "        # if self.arch = [2,3,2] then the lay1_wts have a shape of (3,3) and lay2_wts a shape of (4,2)\n",
        "        for i in range(0, len(self.arch) - 1):\n",
        "          self.layer_wts.append(np.random.randn(self.arch[i] + 1, self.arch[i+1]))\n",
        "        return self.layer_wts\n",
        "\n",
        "    # def initialize_zero_weights(self):\n",
        "    #     \"\"\" Initialize weights for perceptron. Don't forget the bias!\n",
        "\n",
        "    #     Returns:\n",
        "\n",
        "    #     \"\"\"\n",
        "    #     # if self.arch = [2,3,2] then the lay1_wts have a shape of (3,3) and lay2_wts a shape of (4,2)\n",
        "    #     for i in range(0, len(self.arch) - 1):\n",
        "    #       self.layer_wts.append(np.zeros((self.arch[i] + 1, self.arch[i+1])))\n",
        "    #     return self.layer_wts\n",
        "\n",
        "    def score(self, X, y):\n",
        "        \"\"\" \n",
        "            Return accuracy of model on a given dataset. Must implement own \n",
        "            score function.\n",
        "        Args:\n",
        "            X (array-like): A 2D numpy array with data, excluding targets\n",
        "            y (array-like): A 2D numpy array with targets\n",
        "        Returns:\n",
        "            score : float\n",
        "                Mean accuracy of self.predict(X) wrt. y.\n",
        "        \"\"\"\n",
        "        self._shuffle_data(X,y)\n",
        "        predictions = self.predict(X)\n",
        "        correct = 0\n",
        "        total = y.shape[0]\n",
        "        for i in range(0, y.shape[0]):\n",
        "          if predictions[i][0] == y[i][0]:\n",
        "            correct += 1\n",
        "        return correct/total\n",
        "\n",
        "\n",
        "    def _shuffle_data(self, X, y):\n",
        "        \"\"\" Shuffle the data! This _ prefix suggests that this method should only be called internally.\n",
        "            It might be easier to concatenate X & y and shuffle a single 2D array, rather than\n",
        "             shuffling X and y exactly the same way, independently.\n",
        "        \"\"\"\n",
        "        single_arr = np.concatenate((X,y), axis=1) # concatenate X and y into a single array\n",
        "        np.random.shuffle(single_arr) # shuffle the rows of the concatenated X-y array\n",
        "        cutoff = single_arr.shape[1] - 1 # the point to split the X and y arrays after shuffling\n",
        "        X = single_arr[:,:cutoff] # the shuffled X array\n",
        "        y = single_arr[:,cutoff:] # the shuffled y array\n",
        "\n",
        "    ### Not required by sk-learn but required by us for grading. Returns the weights.\n",
        "    def get_weights(self):\n",
        "        return self.init_layer_wts\n"
      ],
      "metadata": {
        "id": "V9fo9TKBXiwf"
      },
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "array = np.array([[2],[5],[7]])\n",
        "print(array)\n",
        "array2 = np.array([.5, .5])\n",
        "print(array2)\n",
        "print(array*array2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4B7gEtpoR0FV",
        "outputId": "2cead0a2-6893-4e6c-819b-3cef8603a281"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[2]\n",
            " [5]\n",
            " [7]]\n",
            "[0.5 0.5]\n",
            "[[1.  1. ]\n",
            " [2.5 2.5]\n",
            " [3.5 3.5]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KibCIXIThpbE"
      },
      "source": [
        "## 1.1 \n",
        "\n",
        "Debug your model using the following parameters:\n",
        "\n",
        "Learning Rate = 0.1\\\n",
        "Momentum = 0.5\\\n",
        "Deterministic = 10 [This means run it 10 epochs and should be the same everytime you run it]\\\n",
        "Shuffle = False\\\n",
        "Validation size = 0\\\n",
        "Initial Weights = All zeros\\\n",
        "Hidden Layer Widths = [4]\n",
        "\n",
        "---\n",
        "\n",
        "### 1.1.1 Debug \n",
        "\n",
        "Debug your model by running it on the [Debug Dataset](https://byu.instructure.com/courses/14142/files?preview=4421290)\n",
        "\n",
        "\n",
        "Expected Results for Binary Classification (i.e. 1 output node): [debug_bp_0.csv](https://byu.instructure.com/courses/14142/files?preview=4537323) \n",
        "\n",
        "$$ \\text{Layer 1} = \\begin{bmatrix} -8.81779797\\text{e}-05 & -8.81779797\\text{e}-05 & -8.81779797\\text{e}-05 & -8.81779797\\text{e}-05 \\\\ 7.82757731\\text{e}-04 & 7.82757731\\text{e}-04 & 7.82757731\\text{e}-04 & 7.82757731\\text{e}-04 \\\\ -3.94353645\\text{e}-03 & -3.94353645\\text{e}-03 & -3.94353645\\text{e}-03 & -3.94353645\\text{e}-03 \\end{bmatrix}$$\n",
        "                                             \n",
        "$$ \\text{Layer 2} = \\begin{bmatrix} -0.01060888 \\\\ -0.01060888 \\\\ -0.01060888 \\\\ -0.01060888 \\\\ -0.02145495 \\end{bmatrix}$$\n",
        "\n",
        "(The weights do not need to be in this order or shape.)\n",
        "\n",
        "Expected Results for One Hot Vector Classification (i.e. 2 output nodes): [debug_bp_2outs.csv](https://byu.instructure.com/courses/14142/files?preview=4537340) \n",
        "\n",
        "$$ \\text{Layer 1} = \\begin{bmatrix} -0.00018149 & -0.00018149 & -0.00018149 & -0.00018149 \\\\ 0.00157468 & 0.00157468 & 0.00157468 & 0.00157468 \\\\ -0.00788218 & -0.00788218 & -0.00788218 & -0.00788218 \\end{bmatrix}$$\n",
        "                          \n",
        "$$ \\text{Layer 2} = \\begin{bmatrix} 0.01050642 & -0.01050642 \\\\ 0.01050642 & -0.01050642 \\\\ 0.01050642 & -0.01050642 \\\\ 0.01050642 & -0.01050642 \\\\ 0.02148778 & -0.02148778 \\end{bmatrix}$$\n",
        "\n",
        "(The weights do not need to be in this order or shape.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {
        "id": "KgAyy82gixIF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "ee28368c-df2c-4703-df67-3f81cc71fd6b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INITIAL WEIGHTS:\n",
            " [array([[0, 0, 0, 0],\n",
            "       [0, 0, 0, 0],\n",
            "       [0, 0, 0, 0]]), array([[0, 0],\n",
            "       [0, 0],\n",
            "       [0, 0],\n",
            "       [0, 0],\n",
            "       [0, 0]])]\n",
            "\n",
            "\n",
            "FORWARD-FEED:\n",
            "\n",
            "TRAINING INSTANCE # 1\n",
            "Training Instance data:\n",
            " [-0.4  0.3  1. ]\n",
            "weights at layer 0:\n",
            " [[0 0 0 0]\n",
            " [0 0 0 0]\n",
            " [0 0 0 0]]\n",
            "target value for training instance #1:\n",
            " 1.0\n",
            "nets at hidden layer 1:\n",
            " [0. 0. 0. 0.]\n",
            "Zs at hidden layer 1:\n",
            " [0.5 0.5 0.5 0.5]\n",
            "weights at layer 1:\n",
            " [[0 0]\n",
            " [0 0]\n",
            " [0 0]\n",
            " [0 0]\n",
            " [0 0]]\n",
            "target value for training instance #1:\n",
            " 1.0\n",
            "Zs at hidden layer2:\n",
            " [0.5 0.5 0.5 0.5 1. ]\n",
            "nets at hidden layer2:\n",
            " [0. 0.]\n",
            "Zs at hidden layer 2:\n",
            " [0.5 0.5]\n",
            "\n",
            "\n",
            "BACK-PROPAGATE:\n",
            "\n",
            "TARGET VECTOR:\n",
            " [0. 1.]\n",
            "node erros at output layer:\n",
            " [array([-0.125,  0.125])]\n",
            "self.delta_wts:\n",
            " [array([[0, 0, 0, 0],\n",
            "       [0, 0, 0, 0],\n",
            "       [0, 0, 0, 0]]), array([[0, 0],\n",
            "       [0, 0],\n",
            "       [0, 0],\n",
            "       [0, 0],\n",
            "       [0, 0]])]\n",
            "\n",
            "self.delta_wts:\n",
            " [array([[0, 0, 0, 0],\n",
            "       [0, 0, 0, 0],\n",
            "       [0, 0, 0, 0]]), array([[-0.00625,  0.00625],\n",
            "       [-0.00625,  0.00625],\n",
            "       [-0.00625,  0.00625],\n",
            "       [-0.00625,  0.00625],\n",
            "       [-0.0125 ,  0.0125 ]])]\n",
            "dot product of layer weights and node errors from previous layer:\n",
            "TEST self.layer_wts[lay_indx]\n",
            " [[0 0]\n",
            " [0 0]\n",
            " [0 0]\n",
            " [0 0]\n",
            " [0 0]]\n",
            "TEST self.node_errors_by_layer[i-1]\n",
            " [-0.125  0.125]\n",
            "delta_dot_w = \n",
            " [0. 0. 0. 0.]\n",
            "f_prime_net [0.25 0.25 0.25 0.25]\n",
            "self.node_errors_by_layer\n",
            " [array([-0.125,  0.125]), array([0., 0., 0., 0.])]\n",
            "SECOND\n",
            " [array([0., 0., 0., 0.]), array([-0.125,  0.125])]\n",
            "\n",
            "\n",
            "FORWARD-FEED:\n",
            "\n",
            "TRAINING INSTANCE # 2\n",
            "Training Instance data:\n",
            " [-0.3  0.8  1. ]\n",
            "weights at layer 0:\n",
            " [[0 0 0 0]\n",
            " [0 0 0 0]\n",
            " [0 0 0 0]]\n",
            "target value for training instance #2:\n",
            " 1.0\n",
            "nets at hidden layer 1:\n",
            " [0. 0. 0. 0.]\n",
            "Zs at hidden layer 1:\n",
            " [0.5 0.5 0.5 0.5]\n",
            "weights at layer 1:\n",
            " [[0 0]\n",
            " [0 0]\n",
            " [0 0]\n",
            " [0 0]\n",
            " [0 0]]\n",
            "target value for training instance #2:\n",
            " 1.0\n",
            "Zs at hidden layer2:\n",
            " [0.5 0.5 0.5 0.5 1.  1. ]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-84-0763a8476beb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;31m# TRAIN ON DEBUG DATA\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0mmlp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMLP\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m \u001b[0mmlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_2D\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepochs_no_change\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m11\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdefault_wts\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minitial_wts\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-83-3b6e199f7751>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, epochs_no_change, tol, default_wts, initial_wts)\u001b[0m\n\u001b[1;32m     83\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Zs at hidden layer{lay+1}:\\n'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mZs_by_layer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlay\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# this has to be lay - 1 because each element is added per iteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m                 \u001b[0;31m# print(f'weights at layer{lay+1}:\\n', self.layer_wts[lay]) # self.layer_wts[lay][:-1,:] this would get all els except the last one\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m                 \u001b[0mnet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mZs_by_layer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlay\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer_wts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlay\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'nets at hidden layer{lay+1}:\\n'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnets_by_layer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 5 is different from 6)"
          ]
        }
      ],
      "source": [
        "from scipy.io.arff import loadarff \n",
        "import pandas as pd\n",
        "# Load debug data\n",
        "\n",
        "# Train on debug data\n",
        "\n",
        "# Print weights\n",
        "\n",
        "\n",
        "# LOAD DEBUG DATA\n",
        "raw_data = loadarff('/content/drive/MyDrive/School/CS_472_MachLearning/labs/lab2_backpropagation/data/linsep2nonorigin.arff')\n",
        "df_data = pd.DataFrame(raw_data[0])\n",
        "\n",
        "# CHANGE THE VALUES OF THE ROWS OF THE 'class' COLUMN TO BE 1s AND 0s INSTEAD OF \"b'1'\" OR \"b'0'\"\n",
        "df_data['class'] = df_data['class'].astype(int) \n",
        "\n",
        "np_arr = df_data.to_numpy() #cast dataframe to numpy array\n",
        "# np_arr = np.array([[-1,0.4,0.2]])\n",
        "# np_arr = np.array([[.9,.6,0]])\n",
        "cutoff = np_arr.shape[1] - 1 #the index to split the X and y arrays\n",
        "\n",
        "# CUT THE numpy array INTO AN X AND y ARRAY\n",
        "X = np_arr[:,:cutoff] #inputs: from 1st column to cutoff (exclusive)\n",
        "y_2D = np_arr[:,cutoff:] #targets: from cutoff to last column\n",
        "\n",
        "# SET INITIAL WEIGHTS TO ZERO\n",
        "wts = np.zeros((X.shape[1]+1,1)) # weights\n",
        "\n",
        "# TRAIN ON DEBUG DATA\n",
        "mlp = MLP([2,4,2],2,0.1,0)\n",
        "mlp.fit(X,y_2D,epochs_no_change=11,tol=0.01,default_wts=False,initial_wts=0)\n",
        "\n",
        "\n",
        "\n",
        "# PRINT ACCURACY AND WEIGHTS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kY3VNB1ui03N"
      },
      "source": [
        "### 1.1.2 Evaluation\n",
        "\n",
        "Evaluate your model using the following parameters:\n",
        "\n",
        "Learning Rate = 0.1\\\n",
        "Momentum = 0.5\\\n",
        "Deterministic = 10 [This means run it 10 epochs and should be the same everytime you run it]\\\n",
        "Shuffle = False\\\n",
        "Validation size = 0\\\n",
        "Initial Weights = All zeros\\\n",
        "Hidden Layer Widths = [4]\n",
        "\n",
        "We will evaluate your model based on printed weights after training on the [Evaluation Dataset](https://byu.instructure.com/courses/14142/files?preview=4421294)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2yAxA78QjDh2"
      },
      "outputs": [],
      "source": [
        "# Load evaluation data\n",
        "\n",
        "# Train on evaluation data\n",
        "\n",
        "# Print weights"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9vWiTdlbR2Xh"
      },
      "source": [
        "## 2. (10%) Backpropagation on the Iris Classification problem.\n",
        "\n",
        "Load the Iris Dataset [Iris Dataset](https://byu.instructure.com/courses/14142/files?preview=4421369)\n",
        "\n",
        "Parameters:\n",
        "- One layer of hidden nodes with the number of hidden nodes being twice the number of inputs.\n",
        "- Use a 80/20 split of the data for the training/test set.\n",
        "- Use a learning rate of 0.1\n",
        "- Use a validation set (15% of the training set) taken from the training set for your stopping criteria\n",
        "- Create one graph with MSE (mean squared error) over epochs from the training set and validation set\n",
        "- Create one graph with classification accuracy (% classified correctly) over epochs from the training set and validation set\n",
        "- Print out your test set accuracy\n",
        "\n",
        "The results for the different measurables should be shown with a different color, line type, etc. Typical backpropagation accuracies for the Iris data set are 85-95%.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4SSoasDQSKXb"
      },
      "outputs": [],
      "source": [
        "# Iris Classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SIRG42TgSR4x"
      },
      "source": [
        "## 3. (10%) Working with the Vowel Dataset - Learning Rate\n",
        "\n",
        "Load the Vowel Dataset [Vowel Dataset](https://byu.instructure.com/courses/14142/files?preview=4537354)\n",
        "\n",
        "- Use one layer of hidden nodes with the number of hidden nodes being twice the number of inputs.\n",
        "- Use random 80/20 splits of the data for the training/test set.\n",
        "- Use a validation set (15% of the training set) taken from the training set for your stopping criteria\n",
        "- Try some different learning rates (LR). Note that each LR will probably require a different number of epochs to learn. \n",
        "\n",
        "- For each LR you test, plot their validation's set MSE over Epochs on the same graph. Graph 4-5 different LRs and make them different enough to see a difference between them.\n",
        "\n",
        "In general, whenever you are testing a parameter such as LR, # of hidden nodes, etc., test values until no more improvement is found. For example, if 20 hidden nodes did better than 10, you would not stop at 20, but would try 40, etc., until you no longer get improvement.\n",
        "\n",
        "If you would like you may average the results of multiple initial conditions (e.g. 3) per LR, and that obviously would give more accurate results.\n",
        "\n",
        "<img src=https://raw.githubusercontent.com/cs472ta/CS472/master/images/backpropagation/backprop_val_set_MSE_vs_epochs.png width=500 height=500  align=\"left\">"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KBGUn43ASiXW"
      },
      "outputs": [],
      "source": [
        "# Train on each dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mOteTlV6S0bq"
      },
      "source": [
        "## 3.1 (5%) Working with the Vowel Dataset - Intuition\n",
        "- Discuss the effect of varying learning rates. \n",
        "- Discuss why the vowel data set might be more difficult than Iris\n",
        "    - Report both datasets' baseline accuracies and best **test** set accuracies. \n",
        "- Consider which of the vowel dataset's given input features you should actually use (Train/test, speaker, gender, ect) and discuss why you chose the ones you did.\n",
        "\n",
        "Typical backpropagation accuracies for the Vowel data set are above 75%.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qmq9GSbJS8k2"
      },
      "source": [
        "*Discuss Intuition here*\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LCnyi7wtBTCM"
      },
      "source": [
        "## 3.2 (10%) Working with the Vowel Dataset - Hidden Layer Nodes\n",
        "\n",
        "Using the best LR you discovered, experiment with different numbers of hidden nodes.\n",
        "\n",
        "- Start with 1 hidden node, then 2, and then double them for each test until you get no more improvement in accuracy. \n",
        "- For each number of hidden nodes find the best validation set solution (in terms of validation set MSE).  \n",
        "- Create one graph with MSE for the training set and validation set on the y-axis and # of hidden nodes on the x-axis.\n",
        "- Report the final test set accuracy for every # of hidden nodes you experimented on"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V2JTuao6BTCM"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oXB_e0-5BTCM"
      },
      "source": [
        "*Discuss Hidden Layer Nodes here*\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eYuhn1qsBTCM"
      },
      "source": [
        "## 3.3 (10%) Working with the Vowel Dataset - Momentum\n",
        "\n",
        "Try some different momentum terms using the best number of hidden nodes and LR from your earlier experiments.\n",
        "\n",
        "- Create a graph similar to step 3.2, but with momentum on the x-axis and number of epochs until validation set convergence on the y-axis.\n",
        "- For each momentum term, print the test set accuracy. \n",
        "- You are trying to see how much momentum speeds up learning and how it affects accuracy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e8hMcc3kBTCN"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WEgtEY81BTCN"
      },
      "source": [
        "*Discuss Momentum here*\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oBBmeNQ7jvcQ"
      },
      "source": [
        "## 4.1 (10%) Use the scikit-learn (SK) version of the MLP classifier on the Iris and Vowel data sets.  \n",
        "\n",
        "You do not need to go through all the steps above, nor graph results. Compare results (accuracy and learning speed) between your version and theirs for some selection of hyper-parameters. Try different hyper-parameters and comment on their effect.\n",
        "\n",
        "At a minimum, try\n",
        "\n",
        "- number of hidden nodes and layers\n",
        "- different activation functions\n",
        "- learning rate\n",
        "- regularization and parameters\n",
        "- momentum (and try nesterov)\n",
        "- early stopping"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OFQv70W2VyqJ"
      },
      "outputs": [],
      "source": [
        "# Load sklearn perceptron\n",
        "\n",
        "# Train on voting dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BqSFAXwlk3Ms"
      },
      "source": [
        "*Record impressions*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AmDm8gnCBTCO"
      },
      "source": [
        "## 4.2 (5%) Using the Iris Dataset automatically adjust hyper-parameters using your choice of grid/random search\n",
        "- Use a grid or random search approach across a reasonable subset of hyper-parameters from the above \n",
        "- Report your best accuracy and hyper-parameters. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jAyUtXh0BTCO"
      },
      "outputs": [],
      "source": [
        "# Load sklearn perceptron\n",
        "\n",
        "# Train on voting dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cTlK-kijk8Mg"
      },
      "source": [
        "## 5. (Optional 5% Extra credit) For the vowel data set, use the other hyper-parameter approach that you did not use in part 4.2 to find LR, # of hidden nodes, and momentum.  \n",
        "\n",
        "- Compare and discuss the values found with the ones you found in part 3.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m9KatPD7BTCP"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ps3oqZdBBTCP"
      },
      "source": [
        "*Discuss findings here*"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "lab_2_backprop.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}